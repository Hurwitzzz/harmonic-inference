{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a191b7ca072ee042befe415e9c73f471d3b801044783c4feae37afcfb1965ba9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from harmonic_inference.data.piece import Piece, ScorePiece\n",
    "import harmonic_inference.models.initial_chord_models as icm\n",
    "from harmonic_inference.data.corpus_reading import load_clean_corpus_dfs\n",
    "from harmonic_inference.models.joint_model import MODEL_CLASSES, HarmonicInferenceModel\n",
    "import harmonic_inference.data.datasets as ds\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_df, measures_df, chords_df, notes_df = load_clean_corpus_dfs('corpus_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model paths\n",
    "model_paths = {}\n",
    "\n",
    "for model in MODEL_CLASSES.keys():\n",
    "    if model == \"icm\":\n",
    "        continue\n",
    "\n",
    "    model_paths[model] = os.path.join(\n",
    "        \"checkpoints\", model, \"lightning_logs\", \"version_*\", \"checkpoints\", \"*.ckpt\"\n",
    "    )\n",
    "\n",
    "model_paths[\"icm\"] = os.path.join(\"checkpoints\", \"icm\", \"initial_chord_prior.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "models = {}\n",
    "for model_name, model_class in MODEL_CLASSES.items():\n",
    "    if model_name == \"icm\":\n",
    "        continue\n",
    "\n",
    "    possible_checkpoints = sorted(glob(model_paths[model_name]))\n",
    "    if len(possible_checkpoints) == 0:\n",
    "        logging.error(f\"No checkpoints found for {model_name} in {model_paths[model_name]}\")\n",
    "        sys.exit(2)\n",
    "\n",
    "    if len(possible_checkpoints) == 1:\n",
    "        checkpoint = possible_checkpoints[0]\n",
    "        logging.info(f\"Loading checkpoint {checkpoint} for {model_name}.\")\n",
    "\n",
    "    else:\n",
    "        checkpoint = possible_checkpoints[0]\n",
    "        logging.info(f\"Multiple checkpoints found for {model_name}. Loading {checkpoint}.\")\n",
    "\n",
    "    models[model_name] = model_class.load_from_checkpoint(checkpoint)\n",
    "    models[model_name].freeze()\n",
    "\n",
    "# Load icm json differently\n",
    "logging.info(f\"Loading checkpoint {model_paths['icm']} for icm.\")\n",
    "models[\"icm\"] = icm.SimpleInitialChordModel(model_paths[\"icm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_dir = Path(\"h5_data\")\n",
    "seed = 0\n",
    "\n",
    "# Load validation data for ctm\n",
    "h5_path = Path(h5_dir / f\"ChordTransitionDataset_valid_seed_{seed}.h5\")\n",
    "with h5py.File(h5_path, \"r\") as h5_file:\n",
    "    if \"file_ids\" not in h5_file:\n",
    "        logging.error(f\"file_ids not found in {h5_path}. Re-create with create_h5_data.py\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    file_ids = list(h5_file[\"file_ids\"])\n",
    "\n",
    "# Load from pkl if available\n",
    "pkl_path = Path(h5_dir / f\"pieces_valid_seed_{seed}.pkl\")\n",
    "if pkl_path.exists():\n",
    "    with open(pkl_path, \"rb\") as pkl_file:\n",
    "        piece_dicts = pickle.load(pkl_file)\n",
    "else:\n",
    "    piece_dicts = [None] * len(file_ids)\n",
    "\n",
    "pieces = [\n",
    "    ScorePiece(\n",
    "        notes_df.loc[file_id],\n",
    "        chords_df.loc[file_id],\n",
    "        measures_df.loc[file_id],\n",
    "        piece_dict=piece_dict,\n",
    "        name=(\n",
    "            f\"{file_id}: {files_df.loc[file_id, 'corpus_name']}/\"\n",
    "            f\"{files_df.loc[file_id, 'file_name']}\"\n",
    "        ),\n",
    "    ) for file_id, piece_dict in tqdm(\n",
    "        zip(file_ids, piece_dicts),\n",
    "        total=len(file_ids),\n",
    "        desc=\"Loading pieces\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_classes = {\n",
    "    \"ccm\": ds.ChordClassificationDataset,\n",
    "    \"ctm\": ds.ChordTransitionDataset,\n",
    "    \"csm\": ds.ChordSequenceDataset,\n",
    "    \"ktm\": ds.KeyTransitionDataset,\n",
    "    \"ksm\": ds.KeySequenceDataset,\n",
    "}\n",
    "\n",
    "# Load and evaluate validation data\n",
    "for model_name, model in models.items():\n",
    "    logging.info(f\"Evaluating {model_name}\")\n",
    "    if model_name == \"icm\":\n",
    "        logging.info(model.evaluate(pieces))\n",
    "        continue\n",
    "\n",
    "    dataset = dataset_classes[model_name]\n",
    "    h5_path_valid = Path(h5_dir / f\"{dataset.__name__}_valid_seed_{seed}.h5\")\n",
    "    dataset_valid = ds.h5_to_dataset(h5_path_valid, dataset, transform=torch.from_numpy)\n",
    "\n",
    "    logging.info(model.evaluate(dataset_valid))"
   ]
  }
 ]
}