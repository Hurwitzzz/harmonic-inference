{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_reading import read_dump\n",
    "import corpus_utils\n",
    "\n",
    "chords_tsv = 'data/chord_list.tsv'\n",
    "notes_tsv = 'data/note_list.tsv'\n",
    "measures_tsv = 'data/measure_list.tsv'\n",
    "files_tsv = 'data/file_list.tsv'\n",
    "\n",
    "chords_df = read_dump(chords_tsv)\n",
    "notes_df = read_dump(notes_tsv, index_col=[0,1,2])\n",
    "measures_df = read_dump(measures_tsv)\n",
    "files_df = read_dump(files_tsv, index_col=0)\n",
    "\n",
    "# Bugfixes\n",
    "measures_df.loc[(685, 487), 'next'][0] = 488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import harmonic_inference_data as hid\n",
    "import importlib\n",
    "importlib.reload(hid)\n",
    "\n",
    "datasets = {'global': {},\n",
    "            'local': {},\n",
    "            'none': {}}\n",
    "\n",
    "datasets['global']['train'], datasets['global']['valid'], datasets['global']['test'] = hid.get_train_valid_test_splits(\n",
    "    chords_df=chords_df, notes_df=notes_df, measures_df=measures_df, files_df=files_df,\n",
    "    seed=0, h5_directory='data', h5_prefix='globalkey_811split', make_dfs=True, transpose_global=True\n",
    ")\n",
    "\n",
    "datasets['local']['train'], datasets['local']['valid'], datasets['local']['test'] = hid.get_train_valid_test_splits(\n",
    "    chords_df=chords_df, notes_df=notes_df, measures_df=measures_df, files_df=files_df,\n",
    "    seed=0, h5_directory='data', h5_prefix='localkey_811split', make_dfs=True, transpose_local=True\n",
    ")\n",
    "\n",
    "datasets['none']['train'], datasets['none']['valid'], datasets['none']['test'] = hid.get_train_valid_test_splits(\n",
    "    chords_df=chords_df, notes_df=notes_df, measures_df=measures_df, files_df=files_df,\n",
    "    seed=0, h5_directory='data', h5_prefix='811split', make_dfs=True\n",
    ")\n",
    "\n",
    "data = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import harmonic_inference_models as him\n",
    "import harmonic_utils\n",
    "import importlib\n",
    "importlib.reload(him)\n",
    "from ablation import get_masks_and_names\n",
    "import torch\n",
    "import os\n",
    "\n",
    "masks, mask_names = get_masks_and_names()\n",
    "dir_name = 'results'\n",
    "\n",
    "index = -1\n",
    "prefix = (data + '_') if data is not 'none' else ''\n",
    "\n",
    "mask = torch.tensor(masks[index])\n",
    "mask_name = mask_names[index]\n",
    "checkpoint_dir = os.path.join(dir_name, prefix + mask_name)\n",
    "log_file_name = os.path.join(dir_name, prefix + mask_name + '.log')\n",
    "\n",
    "model = him.MusicScoreModel(len(train_dataset[0]['notes'][0]), len(harmonic_utils.CHORD_TYPES) * 12,\n",
    "                            dropout=0.2, input_mask=mask)\n",
    "\n",
    "print(mask_name)\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import model_trainer\n",
    "import importlib\n",
    "importlib.reload(model_trainer)\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "schedule_var = 'valid_loss'\n",
    "\n",
    "trainer = model_trainer.ModelTrainer(model, train_dataset=train_dataset, valid_dataset=valid_dataset,\n",
    "                                     test_dataset=test_dataset, seed=0, num_epochs=100, early_stopping=20,\n",
    "                                     optimizer=optimizer, scheduler=scheduler, schedule_var=schedule_var,\n",
    "                                     criterion=criterion,\n",
    "                                     log_every=1, log_file_name=log_file_name,\n",
    "                                     save_every=10, save_dir=checkpoint_dir, save_prefix='checkpoint',\n",
    "                                     resume=os.path.join(checkpoint_dir, 'best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = model_trainer.ModelTrainer(model, train_dataset=train_dataset, valid_dataset=valid_dataset,\n",
    "                                     test_dataset=test_dataset, seed=0, num_epochs=100, early_stopping=20,\n",
    "                                     optimizer=optimizer, scheduler=scheduler, schedule_var=schedule_var,\n",
    "                                     criterion=criterion,\n",
    "                                     log_every=1, log_file_name=log_file_name,\n",
    "                                     save_every=10, save_dir=checkpoint_dir, save_prefix='checkpoint',\n",
    "                                     resume=os.path.join(checkpoint_dir, 'best.pth.tar'))\n",
    "\n",
    "loss, acc, outputs, labels = trainer.evaluate(valid=False)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils as eu\n",
    "import harmonic_utils as hu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_strings = hu.get_one_hot_labels()\n",
    "conf_mat = eu.get_conf_mat(labels, outputs)\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.imshow(conf_mat, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks=list(range(len(label_strings))), labels=label_strings, rotation=90, fontsize=10)\n",
    "plt.yticks(ticks=list(range(len(label_strings))), labels=label_strings, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils as eu\n",
    "\n",
    "correct, incorrect = eu.get_correct_and_incorrect_indexes(labels, outputs)\n",
    "print('Correct: ' + str(len(correct)))\n",
    "print('Incorrect: ' + str(len(incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils as eu\n",
    "    \n",
    "eu.print_result(incorrect[0], labels, outputs, limit=10, prob=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils as eu\n",
    "\n",
    "chord, onset_notes, all_notes = eu.get_input_df_rows(incorrect[0], test_dataset)\n",
    "\n",
    "print(chord)\n",
    "print(\"USED NOTES:\")\n",
    "print(onset_notes)\n",
    "print()\n",
    "print(\"ALL NOTES:\")\n",
    "print(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import eval_utils as eu\n",
    "\n",
    "correct_ranks, indexes_by_rank = eu.get_correct_ranks(labels, outputs)\n",
    "    \n",
    "plt.figure(figsize=(30,30))\n",
    "plt.bar(range(len(outputs[0])), [len(indexes) for indexes in indexes_by_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_utils as eu\n",
    "import importlib\n",
    "importlib.reload(eu)\n",
    "\n",
    "eval_df = eu.get_eval_df(labels, outputs, test_dataset)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ablation\n",
    "import importlib\n",
    "importlib.reload(ablation)\n",
    "\n",
    "dfs = ablation.load_all_ablated_dfs(directory='results', prefix=prefix[:-1] if len(prefix) > 0 else None)\n",
    "_, mask_names = ablation.get_masks_and_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "logs = []\n",
    "for mask_name in mask_names:\n",
    "    logs.append(pd.read_csv(os.path.join(os.path.join('results', prefix + mask_name + '.log'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, log, mask_name in zip(dfs, logs, mask_names):\n",
    "    print(f\"{mask_name} Acc: {100 * df.correct.sum() / len(df)}\")\n",
    "    print(log.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
